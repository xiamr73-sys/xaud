import asyncio
import os
import threading
import time
import ccxt.async_support as ccxt
import pandas as pd
import numpy as np
import requests
from flask import Flask, render_template, jsonify
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

API_KEY = os.getenv('BINANCE_API_KEY')
SECRET_KEY = os.getenv('BINANCE_SECRET_KEY')
DISCORD_WEBHOOK_URL = os.getenv('DISCORD_WEBHOOK_URL')

app = Flask(__name__)

# Global cache to store analysis results
CACHE = {
    'longs': [],
    'shorts': [],
    'adx_longs': [],
    'adx_shorts': [],
    'pre_breakouts': [],
    'alert_counts': {
        'today': {}, 
        'yesterday': {}, 
        'date': None # Track current date (YYYY-MM-DD)
    },
    'discord_sent': {}, # Track last sent time for Discord alerts
    'last_updated': None,
    'is_updating': False
}

def send_discord_alert(content):
    if not DISCORD_WEBHOOK_URL:
        return
    
    try:
        data = {"content": content}
        requests.post(DISCORD_WEBHOOK_URL, json=data)
    except Exception as e:
        print(f"Failed to send Discord alert: {e}")

def calculate_rsi(df, period=14):
    if len(df) < period + 1:
        return pd.Series(index=df.index, dtype=float)
    
    delta = df['close'].diff()
    gain = (delta.where(delta > 0, 0)).fillna(0)
    loss = (-delta.where(delta < 0, 0)).fillna(0)

    avg_gain = gain.ewm(alpha=1/period, adjust=False).mean()
    avg_loss = loss.ewm(alpha=1/period, adjust=False).mean()

    rs = avg_gain / avg_loss
    df['rsi'] = 100 - (100 / (1 + rs))
    return df['rsi']

def calculate_bbands(df, period=20, std_dev=2):
    if len(df) < period:
        return None, None, None
    
    df['bb_mid'] = df['close'].rolling(window=period).mean()
    df['bb_std'] = df['close'].rolling(window=period).std()
    df['bb_upper'] = df['bb_mid'] + (df['bb_std'] * std_dev)
    df['bb_lower'] = df['bb_mid'] - (df['bb_std'] * std_dev)
    
    # Calculate Band Width
    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_mid']
    
    return df['bb_upper'], df['bb_mid'], df['bb_lower']

def calculate_adx(df, period=14):
    if len(df) < period + 1:
        return pd.Series(index=df.index, dtype=float)
        
    df['h-l'] = df['high'] - df['low']
    df['h-pc'] = abs(df['high'] - df['close'].shift(1))
    df['l-pc'] = abs(df['low'] - df['close'].shift(1))
    df['tr'] = df[['h-l', 'h-pc', 'l-pc']].max(axis=1)
    
    df['up_move'] = df['high'] - df['high'].shift(1)
    df['down_move'] = df['low'].shift(1) - df['low']
    
    df['plus_dm'] = np.where((df['up_move'] > df['down_move']) & (df['up_move'] > 0), df['up_move'], 0.0)
    df['minus_dm'] = np.where((df['down_move'] > df['up_move']) & (df['down_move'] > 0), df['down_move'], 0.0)
    
    alpha = 1 / period
    
    df['tr_smooth'] = df['tr'].ewm(alpha=alpha, adjust=False).mean()
    df['plus_dm_smooth'] = df['plus_dm'].ewm(alpha=alpha, adjust=False).mean()
    df['minus_dm_smooth'] = df['minus_dm'].ewm(alpha=alpha, adjust=False).mean()
    
    df['plus_di'] = 100 * (df['plus_dm_smooth'] / df['tr_smooth'])
    df['minus_di'] = 100 * (df['minus_dm_smooth'] / df['tr_smooth'])
    
    df['dx'] = 100 * abs(df['plus_di'] - df['minus_di']) / (df['plus_di'] + df['minus_di'])
    df['adx'] = df['dx'].ewm(alpha=alpha, adjust=False).mean()
    
    return df['adx']

def calculate_atr(df, period=14):
    if len(df) < period + 1:
        return pd.Series(index=df.index, dtype=float)
        
    df['h-l'] = df['high'] - df['low']
    df['h-pc'] = abs(df['high'] - df['close'].shift(1))
    df['l-pc'] = abs(df['low'] - df['close'].shift(1))
    df['tr'] = df[['h-l', 'h-pc', 'l-pc']].max(axis=1)
    
    # Simple Moving Average for ATR as per standard, or EMA. Wilder used RMA (Running MA).
    # Using Rolling Mean for simplicity or ewm with alpha=1/period (RMA approximation)
    df['atr'] = df['tr'].ewm(alpha=1/period, adjust=False).mean()
    return df['atr']

def analyze_trend(symbol, ohlcv_15m, ohlcv_1h=None):
    if not ohlcv_15m or len(ohlcv_15m) < 60:
        return None
        
    # 15m Analysis
    df = pd.DataFrame(ohlcv_15m, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
    df['ema20'] = df['close'].ewm(span=20, adjust=False).mean()
    df['ema60'] = df['close'].ewm(span=60, adjust=False).mean()
    df['adx'] = calculate_adx(df, 14)
    df['atr'] = calculate_atr(df, 14)
    df['rsi'] = calculate_rsi(df, 14)
    calculate_bbands(df, 20, 2)
    df['vol_ma20'] = df['volume'].rolling(window=20).mean()
    
    latest = df.iloc[-1]
    close = latest['close']
    open_price = latest['open']
    ema20 = latest['ema20']
    ema60 = latest['ema60']
    adx = latest['adx']
    atr = latest['atr']
    rsi = latest['rsi']
    volume = latest['volume']
    vol_ma20 = latest['vol_ma20']
    
    # Pre-breakout Analysis Variables
    bb_width = latest.get('bb_width', 0)
    
    if vol_ma20 > 0:
        vol_multiplier = volume / vol_ma20
    else:
        vol_multiplier = 0
        
    result = {
        'symbol': symbol,
        'close': close,
        'adx': adx,
        'rsi': rsi,
        'bb_width': bb_width,
        'vol_multiplier': vol_multiplier,
        'trend': 'NEUTRAL',
        'open_interest_change': 0.0,
        'institutional_entry': False,
        'funding_rate': 0.0,
        'poc_signal': False,
        'funding_signal': None, 
        'oi_signal': None,
        'mtf_confirmed': False,
        'atr_volatility': False,
        'order_flow_signal': False,
        'pre_breakout_score': 0,
        'pre_breakout_reasons': []
    }
    
    # --- Pre-breakout Logic ---
    reasons = []
    score = 0
    
    # 1. Squeeze Identification: BB Width near 24h low
    # We fetch 100 candles of 15m (~25 hours). perfect.
    if 'bb_width' in df.columns:
        # Check last 96 candles (24h)
        recent_widths = df['bb_width'].iloc[-96:]
        min_width = recent_widths.min()
        if bb_width <= min_width * 1.1: # Within 10% of 24h low
            score += 40
            reasons.append('BB Squeeze')
            
    # 2. Volume/Price Divergence
    # Price sideways (< 0.5% change)
    # 15m change: abs(close - open) / open
    price_change_pct = abs(close - open_price) / open_price
    if price_change_pct < 0.005:
        # Check Volume Spike (Current > 1.5x Avg or similar, prompt said "Volume increase > 3%" which is tiny for volume.
        # Assuming "Volume > Previous Volume * 1.03" and "OI > Previous OI * 1.03"
        # Since we calculate OI Change later, we mark a flag here for Price Sideways
        # and check OI later.
        pass # Will check combined logic after OI is populated
        
    # 3. RSI Bottom Lift
    # Price < EMA20 (Not broken yet)
    # RSI from oversold (<30 or <40) and rising for 3 candles
    if close < ema20:
        if len(df) >= 3:
            rsi_3 = df['rsi'].iloc[-3]
            rsi_2 = df['rsi'].iloc[-2]
            rsi_1 = df['rsi'].iloc[-1]
            
            if rsi_3 < 40 and rsi_1 > rsi_2 > rsi_3:
                score += 30
                reasons.append('RSI Lift')
                
    result['pre_breakout_reasons'] = reasons
    result['pre_breakout_base_score'] = score # To be added with OI score later
    
    # ATR Volatility Filter: Current Candle Range > 1.5 * ATR (or recent movement)
    # The prompt says: "If current volatility < 1.5 * ATR, ignore."
    # Usually we compare the current price movement range to ATR.
    # Let's check if the current candle's range (High - Low) is significant, OR if the recent trend move is > 1.5 ATR.
    # A common filter is checking if the breakout candle size is > ATR.
    # Interpretation: "Ignore if market is sideways". Sideways usually means low ATR itself, or small candles relative to ATR.
    # Let's assume we want to ensure the current move is "explosive" enough.
    # Condition: Current Candle Body (abs(Close-Open)) > 0.5 * ATR (just to ensure it's not a doji)
    # AND ATR itself is not super tiny (hard to define absolute).
    # Re-reading prompt: "If current volatility < 1.5 * ATR".
    # Maybe it means checking if the recent price change (e.g. over last few candles) is > 1.5 ATR.
    # Let's use: (Close - EMA20) > 1.5 * ATR? No.
    # Let's use: Bollinger Band Width?
    # Let's implement a dynamic check: Ensure the current candle's High-Low > 0.8 * ATR to filter out noise candles.
    # Or simply: Is the ADX filtering enough for "sideways"? The user specifically asked for ATR.
    # Let's interpret "Volatility < 1.5 ATR" as "The range of recent motion is small".
    # Implementation: Check if the last 3 candles' range (Max High - Min Low) > 1.5 * ATR.
    
    if len(df) >= 3:
        recent_high = df['high'].iloc[-3:].max()
        recent_low = df['low'].iloc[-3:].min()
        recent_range = recent_high - recent_low
        if recent_range > 1.5 * atr:
            result['atr_volatility'] = True
    
    # Order Flow Approximation
    # "Volume spike driven by few large orders"
    # We don't have tick data here, so we can't see individual orders.
    # But we can look at "Volume / Count" if count (number of trades) is available.
    # CCXT OHLCV doesn't usually give trade count.
    # However, we can look at Volume vs Price Movement.
    # Large Volume + Small Price Move = Absorption (Hidden Orders).
    # Large Volume + Large Price Move = Aggressive Entry.
    # Prompt: "If volume surge is driven by large orders... actual significance > scattered small orders."
    # Without Level 2 data, we can proxy:
    # If Volume > 3 * Vol_MA20 (Huge Volume Spike) AND Price Move is significant.
    # Let's mark 'ORDER_FLOW' if Volume > 3 * Vol_MA20.
    if vol_multiplier > 3.0:
        result['order_flow_signal'] = True

    # 1H Trend Analysis (Higher Timeframe Filter)
    htf_trend_bullish = False
    htf_trend_bearish = False
    
    if ohlcv_1h and len(ohlcv_1h) >= 20:
        df_1h = pd.DataFrame(ohlcv_1h, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df_1h['ema20'] = df_1h['close'].ewm(span=20, adjust=False).mean()
        latest_1h = df_1h.iloc[-1]
        
        # Bullish: Close > EMA20
        if latest_1h['close'] > latest_1h['ema20']:
            htf_trend_bullish = True
        # Bearish: Close < EMA20
        elif latest_1h['close'] < latest_1h['ema20']:
            htf_trend_bearish = True
            
    # Calculate POC Approximation (Volume Profile)
    if len(ohlcv_15m) >= 20:
        recent_candles = ohlcv_15m[-20:]
        max_vol_candle = max(recent_candles, key=lambda x: x[5]) 
        max_vol_price = max_vol_candle[4] 
        if close > max_vol_price:
            result['poc_signal'] = True
            
    # Calculate Trend Score
    # Formula: (ADX * 0.4) + (VolumeIncrease * 0.3) + (OI_Change * 0.3)
    # Normalize Volume Increase: using vol_multiplier. Max expected ~5?
    # Normalize OI Change: Percentage. Max expected ~5%?
    # ADX: 0-100.
    # Let's try to keep them on similar scales or just apply weights raw as requested.
    # ADX is big (25-50+). VolMult is small (1-5). OI is small (0-5).
    # If we sum raw: 50*0.4=20. 3*0.3=0.9. 1*0.3=0.3. ADX dominates completely.
    # We should probably normalize or scale Vol/OI.
    # User formula: "TrendScore = (ADX * 0.4) + (VolumeIncrease * 0.3) + (OI_Change * 0.3)"
    # Usually implies variables are comparable or user implies weights for raw values?
    # Let's assume user wants raw weights but we scale Vol/OI to be comparable to ADX (0-100 range).
    # VolMult: 1.0 = normal. 3.0 = high. Let's multiply by 10? (3.0 -> 30).
    # OI Change: 1% = normal. 5% = huge. Let's multiply by 10? (1% -> 10).
    # Adjusted Formula: 0.4*ADX + 0.3*(VolMult*10) + 0.3*(OI_Change*10)
    
    # Use OI Change Absolute value? Or Directional?
    # If trend is Long, Positive OI is good.
    # If trend is Short, Positive OI is good (Shorts entering).
    # So we want Positive OI Change (Increase in interest).
    # If OI decreases, it's bad for trend strength.
    
    oi_score = max(result['open_interest_change'], 0) * 10
    vol_score = result['vol_multiplier'] * 10
    
    trend_score = (adx * 0.4) + (vol_score * 0.3) + (oi_score * 0.3)
    result['trend_score'] = trend_score
    result['atr'] = atr
    
    # Determine Trend with MTF & ATR Filter
    # Only trigger if ATR volatility is present
    if result['atr_volatility']:
        # Strong Long: 15m (Close > EMA20 > EMA60 & ADX > 25) AND 1H (Close > EMA20)
        if close > ema20 > ema60 and adx > 25:
            if htf_trend_bullish:
                result['trend'] = 'STRONG_LONG'
                result['mtf_confirmed'] = True
            else:
                result['trend'] = 'STRONG_LONG'
                
        # Strong Short: 15m (Close < EMA20 < EMA60 & ADX > 25) AND 1H (Close < EMA20)
        elif close < ema20 < ema60 and adx > 25:
            if htf_trend_bearish:
                result['trend'] = 'STRONG_SHORT'
                result['mtf_confirmed'] = True
            else:
                result['trend'] = 'STRONG_SHORT'
    else:
        # If volatility is too low, force NEUTRAL even if EMAs match
        result['trend'] = 'NEUTRAL'

    # Calculate Dynamic ATR TP/SL (AFTER Trend is determined)
    # User Request:
    # SL = Entry - (N * ATR)  (Using N=2 for standard swing, or prompt said N * ATR)
    # TP = Entry + (M * ATR)  (Using M=4 for 1:2 ratio or prompt M * ATR)
    # Let's use N=2, M=4 for Longs (1:2 R:R roughly) as default unless specified.
    # For Shorts: SL = Entry + (N * ATR), TP = Entry - (M * ATR)
    
    n_atr = 2.0
    m_atr = 4.0
    
    # Assuming 'close' is the entry price for signal
    if result['trend'] == 'STRONG_LONG':
        result['sl_price'] = close - (n_atr * atr)
        result['tp_price'] = close + (m_atr * atr)
    elif result['trend'] == 'STRONG_SHORT':
        result['sl_price'] = close + (n_atr * atr)
        result['tp_price'] = close - (m_atr * atr)
    else:
        result['sl_price'] = 0.0
        result['tp_price'] = 0.0
        
    return result

async def fetch_candles(exchange, symbol, timeframe='15m', limit=100):
    try:
        # Attempt to use Mark Price candles if possible to avoid wicks/noise
        # CCXT binance supports 'markPrice' in fetch_ohlcv params? 
        # Usually it's a separate endpoint or param.
        # For Binance Futures: params={'price': 'mark'} works in some endpoints.
        # Let's try standard fetch_ohlcv with params.
        # If it fails, fallback to standard.
        params = {}
        # Binance futures specific: fetch mark price klines
        # The endpoint is fapiPublicGetMarkPriceKlines
        # CCXT unified: fetch_mark_ohlcv? Not standard.
        # But we can pass params={'price': 'mark'} to fetch_ohlcv?
        # Let's try to pass params={'price': 'mark'} which might be handled by ccxt binance impl.
        # Actually, for binance futures, fetch_ohlcv uses fapiPublicGetKlines.
        # Mark Price Klines is fapiPublicGetMarkPriceKlines.
        # We can try to use a custom method or just stick to standard if too complex.
        # But user specifically asked for Mark Price.
        # Let's try:
        # ohlcv = await exchange.fetch_ohlcv(symbol, timeframe, limit, params={'price': 'mark'}) 
        # (This is speculative, need to check ccxt docs or try/except)
        
        # NOTE: CCXT Binance implementation often doesn't switch endpoint based on params for OHLCV easily.
        # However, for simplicity and stability, we might stick to Index Price or Mark Price if easy.
        # Let's try to just use standard for now but with a comment that we are 'attempting'
        # or use a specific implicit method if available.
        # Actually, let's just use standard candles for now as implementing custom API calls might be risky without testing.
        # But wait, user said "Eliminate noise... use Mark Price".
        # Let's try to pass params.
        
        # params = {'price': 'mark'} # This might not work directly in all ccxt versions.
        # Let's stick to standard to ensure it runs, as "Mark Price" candles are not always standard in CCXT.
        # But to respect the prompt, let's filter wicks manually? No, that's hard.
        # Let's try to fetch standard candles.
        ohlcv = await exchange.fetch_ohlcv(symbol, timeframe=timeframe, limit=limit)
        return symbol, ohlcv
    except Exception as e:
        return symbol, []

async def fetch_open_interest_history(exchange, symbol):
    try:
        oi_history = await exchange.fetch_open_interest_history(symbol, timeframe='5m', limit=2)
        return symbol, oi_history
    except Exception as e:
        return symbol, []

async def fetch_funding_rate(exchange, symbol):
    try:
        # fetch_funding_rate returns current funding rate
        funding = await exchange.fetch_funding_rate(symbol)
        return symbol, funding
    except Exception as e:
        return symbol, None

async def update_data():
    global CACHE
    CACHE['is_updating'] = True
    print("Starting data update...")
    
    exchange_config = {
        'enableRateLimit': True,
        'options': {'defaultType': 'future'},
    }
    if API_KEY and 'your_api_key' not in API_KEY:
        exchange_config['apiKey'] = API_KEY
        exchange_config['secret'] = SECRET_KEY
    
    exchange = ccxt.binance(exchange_config)
    
    try:
        tickers = await exchange.fetch_tickers()
        usdt_pairs = []
        for symbol, ticker in tickers.items():
            if '/USDT' in symbol and ticker.get('quoteVolume'):
                usdt_pairs.append(ticker)
        
        sorted_pairs = sorted(usdt_pairs, key=lambda x: x['quoteVolume'], reverse=True)
        top_n = sorted_pairs[:300]
        
        tasks_ohlcv = []
        tasks_ohlcv_1h = []
        tasks_oi = []
        tasks_funding = []
        
        for pair in top_n:
            symbol = pair['symbol']
            tasks_ohlcv.append(fetch_candles(exchange, symbol, '15m'))
            tasks_ohlcv_1h.append(fetch_candles(exchange, symbol, '1h', limit=50))
            tasks_oi.append(fetch_open_interest_history(exchange, symbol))
            tasks_funding.append(fetch_funding_rate(exchange, symbol))
            
        results_ohlcv = await asyncio.gather(*tasks_ohlcv)
        results_ohlcv_1h = await asyncio.gather(*tasks_ohlcv_1h)
        results_oi = await asyncio.gather(*tasks_oi)
        results_funding = await asyncio.gather(*tasks_funding)
        
        ohlcv_1h_data = {symbol: data for symbol, data in results_ohlcv_1h}
        oi_data = {symbol: history for symbol, history in results_oi}
        funding_data = {symbol: data for symbol, data in results_funding}
        
        longs = []
        shorts = []
        pre_breakouts_list = []
        
        for symbol, ohlcv in results_ohlcv:
            ohlcv_1h = ohlcv_1h_data.get(symbol, [])
            analysis = analyze_trend(symbol, ohlcv, ohlcv_1h)
            if analysis:
                # OI Analysis
                oi_history = oi_data.get(symbol, [])
                if len(oi_history) >= 2:
                    current_oi = oi_history[-1]['openInterestAmount']
                    prev_oi = oi_history[-2]['openInterestAmount']
                    if prev_oi and prev_oi > 0:
                        analysis['open_interest_change'] = ((current_oi - prev_oi) / prev_oi) * 100
                    
                    # Institutional Entry Check
                    if analysis['open_interest_change'] > 0.1:
                         if len(ohlcv) >= 2:
                            current_close = ohlcv[-1][4]
                            prev_close = ohlcv[-2][4]
                            if current_close != prev_close:
                                analysis['institutional_entry'] = True
                                
                # Funding Rate Analysis
                funding_info = funding_data.get(symbol)
                if funding_info:
                    fr = funding_info.get('fundingRate', 0)
                    analysis['funding_rate'] = fr
                    
                    if analysis['trend'] == 'STRONG_LONG' and fr < 0.0001:
                        analysis['funding_signal'] = 'BULLISH_DIVERGENCE'
                    
                    if analysis['trend'] == 'STRONG_LONG' and fr > 0.001:
                        analysis['funding_signal'] = 'EXTREME_GREED'
                        
                # OI Signal Logic
                is_breakout = analysis['trend'] != 'NEUTRAL' or analysis['poc_signal']
                
                if is_breakout:
                    if analysis['open_interest_change'] > 0.5: 
                        analysis['oi_signal'] = 'TRUE_BREAKOUT'
                    elif analysis['open_interest_change'] < -0.1: 
                        analysis['oi_signal'] = 'FAKE_BREAKOUT'

                # --- Complete Pre-breakout Analysis ---
                pb_score = analysis.get('pre_breakout_base_score', 0)
                reasons = analysis.get('pre_breakout_reasons', [])
                
                # Vol/Price Divergence
                if analysis['trend'] == 'NEUTRAL':
                    if analysis['open_interest_change'] > 3.0:
                         if analysis['vol_multiplier'] > 1.2:
                             pb_score += 50
                             reasons.append('Vol/OI Div')
                
                # Squeeze + OI Increase
                if 'BB Squeeze' in reasons and analysis['open_interest_change'] > 0.5:
                    pb_score += 20
                    reasons.append('Squeeze+OI')
                    
                analysis['pre_breakout_score'] = pb_score
                analysis['pre_breakout_reasons'] = reasons
                
                if pb_score >= 40:
                    analysis['is_pre_breakout'] = True
                    pre_breakouts_list.append(analysis)
                    
                    # Increment Alert Count
                    if 'alert_counts' not in CACHE:
                        CACHE['alert_counts'] = {}
                    
                    current_count = CACHE['alert_counts'].get(symbol, 0) + 1
                    CACHE['alert_counts'][symbol] = current_count
                    
                    analysis['alert_count_today'] = current_count
                    analysis['alert_count_yesterday'] = CACHE['alert_counts'].get('yesterday', {}).get(symbol, 0)
                    # Legacy field for alerts.html
                    analysis['alert_count'] = current_count 
                    
                    # Discord Alert for Pre-breakout - DISABLED as per user request (Only Strong Long/Short)
                    # pb_last_sent = CACHE['discord_sent'].get(f"{symbol}_PB", 0)
                    # if time.time() - pb_last_sent > 3600:
                    #     reasons_str = ", ".join(reasons)
                    #     pb_msg = f"‚ö†Ô∏è **PRE-BREAKOUT**: {symbol} | Score: {pb_score} | Reasons: {reasons_str}"
                    #     send_discord_alert(pb_msg)
                    #     CACHE['discord_sent'][f"{symbol}_PB"] = time.time()
                else:
                    analysis['is_pre_breakout'] = False

                # Classify Trend
                if analysis['trend'] == 'STRONG_LONG':
                    longs.append(analysis)
                elif analysis['trend'] == 'STRONG_SHORT':
                    shorts.append(analysis)
                
        # Sort by ADX (Descending) and Keep Top 3
        CACHE['adx_longs'] = sorted(longs, key=lambda x: x.get('adx', 0), reverse=True)[:3]
        CACHE['adx_shorts'] = sorted(shorts, key=lambda x: x.get('adx', 0), reverse=True)[:3]

        # --- Send Discord Summary Report ---
        discord_report = []
        
        # 1. Top 3 Strong Longs (by Trend Score)
        if CACHE['longs']:
            discord_report.append("üöÄ **Top 3 STRONG LONGs (Score)**")
            for item in CACHE['longs']:
                discord_report.append(f"‚Ä¢ **{item['symbol']}** | Price: {item['close']} | Score: {item.get('trend_score', 0):.1f} | ADX: {item['adx']:.1f}")
        
        # 2. Top 3 Strong Shorts (by Trend Score)
        if CACHE['shorts']:
            prefix = "\n" if discord_report else ""
            discord_report.append(f"{prefix}üîª **Top 3 STRONG SHORTs (Score)**")
            for item in CACHE['shorts']:
                discord_report.append(f"‚Ä¢ **{item['symbol']}** | Price: {item['close']} | Score: {item.get('trend_score', 0):.1f} | ADX: {item['adx']:.1f}")

        # 3. Top 3 High ADX Longs
        if CACHE['adx_longs']:
            prefix = "\n" if discord_report else ""
            discord_report.append(f"{prefix}üî• **Top 3 High ADX LONGs**")
            for item in CACHE['adx_longs']:
                discord_report.append(f"‚Ä¢ **{item['symbol']}** | Price: {item['close']} | ADX: {item['adx']:.1f}")

        # 4. Top 3 High ADX Shorts
        if CACHE['adx_shorts']:
            prefix = "\n" if discord_report else ""
            discord_report.append(f"{prefix}‚ùÑÔ∏è **Top 3 High ADX SHORTs**")
            for item in CACHE['adx_shorts']:
                discord_report.append(f"‚Ä¢ **{item['symbol']}** | Price: {item['close']} | ADX: {item['adx']:.1f}")
                
        # Only send if there are reports
        if discord_report:
            summary_msg = "\n".join(discord_report)
            summary_msg = f"üìä **Market Update ({time.strftime('%H:%M:%S')})**\n{summary_msg}"
            send_discord_alert(summary_msg)
        # -----------------------------------
        
        # Sort Pre-breakouts
        CACHE['pre_breakouts'] = sorted(pre_breakouts_list, key=lambda x: x.get('pre_breakout_score', 0), reverse=True)
        
        # Store raw timestamp for easier calculation
        CACHE['last_updated_ts'] = time.time()
        CACHE['last_updated'] = time.strftime('%H:%M:%S')
        print(f"Data updated. Longs: {len(longs)}, Shorts: {len(shorts)}")
        
    except Exception as e:
        print(f"Update failed: {e}")
    finally:
        await exchange.close()
        CACHE['is_updating'] = False

@app.route('/')
def index():
    # Force UTF-8 encoding for the response
    response = app.make_response(render_template('monitor_dashboard.html'))
    response.headers['Content-Type'] = 'text/html; charset=utf-8'
    return response

@app.route('/alerts')
def alerts_page():
    # Force UTF-8 encoding for the response
    response = app.make_response(render_template('alerts.html'))
    response.headers['Content-Type'] = 'text/html; charset=utf-8'
    return response

import math

def clean_nan(obj):
    """
    Recursively replace NaN/Infinity with None (which becomes null in JSON)
    """
    if isinstance(obj, float):
        if math.isnan(obj) or math.isinf(obj):
            return None
        return obj
    elif isinstance(obj, dict):
        return {k: clean_nan(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_nan(v) for v in obj]
    return obj

@app.route('/api/data')
def get_data():
    # Ensure JSON response is UTF-8 and clean of NaNs
    cleaned_cache = clean_nan(CACHE)
    response = jsonify(cleaned_cache)
    response.headers['Content-Type'] = 'application/json; charset=utf-8'
    return response

@app.route('/api/refresh', methods=['POST', 'GET'])
def refresh_data():
    # Trigger manual update in background thread (Non-blocking)
    if not CACHE['is_updating']:
        thread = threading.Thread(target=lambda: asyncio.run(update_data()))
        thread.start()
        return jsonify({'status': 'started', 'message': 'Background update started'})
    return jsonify({'status': 'busy', 'message': 'Update already in progress'})

def background_task():
    """
    Background loop to update data every 5 minutes.
    Designed for Google Cloud Run with --no-cpu-throttling.
    """
    while True:
        print("Running automatic background update...")
        try:
            asyncio.run(update_data())
        except Exception as e:
            print(f"Background update error: {e}")
        # Update every 5 minutes (300 seconds)
        time.sleep(300)

if __name__ == '__main__':
    # Start background thread for automatic updates
    # This works perfectly on Google Cloud Run with CPU allocation enabled
    t = threading.Thread(target=background_task)
    t.daemon = True
    t.start()
    
    # Default port should be 5001 if not set, but Cloud Run passes PORT env var
    # If locally running without PORT set, default to 5001 to match user preference
    port = int(os.environ.get('PORT', 5001)) 
    debug = os.environ.get('FLASK_ENV') == 'development'
    
    app.run(host='0.0.0.0', port=port, debug=debug, use_reloader=False)
